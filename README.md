# Bitcoin
## 1. Descripci√≥n General

La clase `DataExtractor` fue dise√±ada para:

1. Cargar datos desde un archivo CSV (posiblemente muy grande) en modo por *chunks* (bloques).
2. Limpiar y normalizar el texto, **manteniendo** hashtags.
3. Extraer hashtags de cada texto.
4. Calcular la frecuencia de uso de los hashtags de forma global, por usuario y por fecha.
5. Generar una nube de palabras (WordCloud) a partir de los hashtags m√°s frecuentes.

El objetivo principal es facilitar el an√°lisis de hashtags en un conjunto grande de publicaciones o comentarios de usuarios (p.e. tuits, posts de redes sociales, foros, etc.).

---

## 2. Fuente de Datos Utilizada

- **Formato CSV**: Se espera que el archivo de origen (source_file) sea un CSV con, al menos, las columnas:
  - `text`: El texto publicado por el usuario (puede contener hashtags).
  - `date`: Fecha y hora aproximada de la publicaci√≥n.
  - `user_name`: El nombre de usuario o identificador de quien public√≥ el texto.

- **Separador**: Coma (`,`).
- **Encabezados Requeridos**: `user_name`, `text`, `date` (como m√≠nimo).
- **Lectura por Bloques**: Se usa el par√°metro `chunksize` para leer el archivo en porciones de 100,000 filas, lo cual ayuda a evitar problemas de memoria en archivos muy grandes.

# 3. Metodolog√≠a de Extracci√≥n, Limpieza y An√°lisis

## 3.1 Extracci√≥n de Datos

### Instancia de la clase:

```python
extractor = DataExtractor(source_file='ruta/al/archivo.csv', chunksize=100000)
```

source_file: Ruta completa al archivo CSV.
chunksize: Cantidad de filas que se leen por bloque (por defecto 100,000).
```python 
extractor.load_data()
```
Se utiliza pandas.read_csv con sep=',', on_bad_lines='skip' y encoding='utf-8'.
Almacena la referencia a los datos en self.data, que ser√° un iterador de chunks de pandas.

## 3.2 Limpieza de Texto
- Se convierte el texto a min√∫sculas.
- Se eliminan URLs (http, https, www).
- Se eliminan caracteres especiales, excepto #, para preservar los hashtags.
- Se quitan espacios redundantes y secuencias de puntos repetidos.
- La funci√≥n clean_text realiza estos pasos, devolviendo una cadena de texto limpio.

## 3.3 Extracci√≥n de Hashtags
Mediante extract_hashtags, se usa una expresi√≥n regular (r'#\w+') para encontrar todas las palabras que inician con #.
Devuelve una lista de hashtags por cada texto.
## 3.4 An√°lisis Avanzado de Hashtags
El m√©todo analytics_hashtags_extended realiza:
- Normalizaci√≥n: Aplica clean_text a cada texto en la columna text.
- Extracci√≥n: Crea una columna Only_hashtag con la lista de hashtags encontrados.
- Conversi√≥n de Fechas: Convierte la columna date para obtener solo la parte de fecha (YYYY-MM-DD), ignorando la hora.
- Explosi√≥n de Hashtags: Usa explode para crear una fila por cada hashtag en Only_hashtag.

### C√°lculo de Frecuencias:

- overall: Frecuencia total de cada hashtag.
- by_user: Frecuencia de cada hashtag por usuario.
- by_date: Frecuencia de cada hashtag por d√≠a.
- Retorna un diccionario con estos DataFrames, adem√°s del data_exploded con toda la informaci√≥n.

# Ejecuci√≥n 
Entrar al proyecto y ejecutar el comando python main.py. Realizara el procedimiento explicado para posteriormente aparecer la imagen generada por  WordCloud.

# Documentaci√≥n de T√©cnicas y Resultados parte II

# Twitter Advanced Text Analysis

Este proyecto implementa un an√°lisis avanzado de textos extra√≠dos desde la API de Twitter a trav√©s de RapidAPI. Incluye extracci√≥n de datos, preprocesamiento, modelado de t√≥picos (LDA), an√°lisis de sentimientos y visualizaci√≥n de estructuras sint√°cticas.

## üìå Fuente de datos

Se utiliza la API "Twitter API v2" disponible en [RapidAPI](https://rapidapi.com/alexanderxbx/api/twitter-api45). Esta API permite realizar b√∫squedas de tweets utilizando par√°metros personalizados como:

- `query`: palabra clave o hashtag a buscar (por ejemplo, `#Tesla`)
- `limit`: n√∫mero m√°ximo de tweets (ej. 200)
- `start`: fecha de inicio (ej. `2025-04-01`)

---

## üß† Metodolog√≠a

### 1. Extracci√≥n de Datos

Se conecta con la API usando la librer√≠a `requests` y se configuran los headers con claves seguras a trav√©s del archivo `.env`.

```python
url = "https://twitter-api45.p.rapidapi.com/search.php"
headers = {
    "X-RapidAPI-Key": os.getenv("RAPIDAPI_KEY"),
    "X-RapidAPI-Host": "twitter-api45.p.rapidapi.com"
}
```

## Modelado de T√≥picos con LDA (Latent Dirichlet Allocation)
- T√©cnica Aplicada: Se aplic√≥ LDA con la librer√≠a gensim para descubrir temas (t√≥picos) dominantes en los tweets. El texto fue previamente limpiado y tokenizado, y se eliminaron stopwords para mejorar la calidad de los temas.
- Justificaci√≥n: LDA permite encontrar grupos de palabras que tienden a aparecer juntas, lo cual revela temas latentes en el corpus de textos. Es muy √∫til en an√°lisis exploratorio de contenido.

Resultado: Se imprimieron 5 t√≥picos. Cada t√≥pico contiene palabras clave con sus pesos. Ejemplo:

üß† T√≥pico 1:
 - "#tesla": 0.051
 - "#": 0.039
 - "you": 0.014
 - "tesla": 0.013
 - "tsla": 0.012
 - "model": 0.008

## An√°lisis de Sentimiento (TextBlob y spaCy)
- T√©cnica Aplicada: Se utiliz√≥ TextBlob (y opcionalmente spaCyTextBlob) para calcular la polaridad y subjetividad de cada texto. Estos valores se guardaron en las columnas sentiment_polarity y sentiment_subjectivity.

- Justificaci√≥n: Ayuda a identificar la actitud (positiva/negativa) de los usuarios hacia los temas comentados.

- Resultado: Se imprimi√≥ un mensaje indicando la finalizaci√≥n del an√°lisis. Adem√°s, se visualiza la distribuci√≥n de polaridad con un histograma:

## Gr√°fico de Distribuci√≥n de Sentimientos:

Se observa la frecuencia de tweets negativos, neutros y positivos.

## Parsing y √Årboles Sint√°cticos
- √©cnica Aplicada: Con spaCy, se carg√≥ un modelo de procesamiento de lenguaje para visualizar la estructura gramatical de una oraci√≥n mediante √°rboles de dependencias (dependency trees).

- Justificaci√≥n: Esto permite entender c√≥mo se relacionan las palabras dentro de una frase, √∫til para an√°lisis sint√°ctico profundo.

- Resultado: Se muestra visualmente un √°rbol sint√°ctico en navegador. Cada palabra conecta con su ‚Äúpalabra padre‚Äù, reflejando la estructura de la oraci√≥n.

## Resumen Extractivo (M√©todo Tradicional con Frecuencia de Palabras)
- T√©cnica Aplicada: Se utiliz√≥ nltk para dividir el texto en oraciones, calcular la frecuencia de palabras y seleccionar las m√°s representativas para formar un resumen.

- Justificaci√≥n: Permite sintetizar los puntos clave del corpus de forma autom√°tica.

- Resultado:  Resumen generado: Se imprime en consola una selecci√≥n de frases relevantes, representativas del contenido total del corpus.

## Intrucciones para reproducir

```
pip install -r requirements.txt
```

Crear archivo .env con tu clave de RapidAPI:

```.env
RAPIDAPI_KEY=tu_clave_aqui
```

ejecutar python main.py.

El proyecto genera :
- Gr√°fico de sentimiento guardado como 'sentiment_distribution.png'
- Tabla de resumen de sentimiento (Por consola)
- √Årbol sint√°ctico con una de las oraciones guardado en dependency_tree.html
- Resumen de los comentarios mas destacados


#  Interacci√≥n y An√°lisis de Redes en Twitter


## 2. Construcci√≥n del Grafo de Interacciones

### M√©todo: `build_interaction_graph`

```python
def build_interaction_graph(self) -> nx.DiGraph
```

Construye un grafo dirigido en el que los nodos representan usuarios, y las aristas indican menciones de un usuario a otro dentro del texto.

####  Requisitos
- `self.data`: debe contener las columnas `user_name` y `text`.
- Los textos se normalizan (min√∫sculas y sin espacios).

####  L√≥gica
- Se filtran textos vac√≠os o nulos.
- Se extraen menciones con regex: `@(\w{1,15})`.
- Se a√±ade un nodo por cada usuario.
- Se crea una arista desde el usuario emisor hacia el usuario mencionado.

####  Ejemplo de uso:
```python
grafo = extractor.build_interaction_graph()
```

####  Resultado:
- Grafo dirigido (`nx.DiGraph`) guardado tambi√©n en `self.graph`.
- Mensaje en consola con el n√∫mero de nodos y aristas.

---

## 3. An√°lisis de la Red

### M√©todo: `analyze_network`

```python
def analyze_network(self, G: nx.DiGraph)
```

Analiza el grafo generado con m√©tricas de red y detecta comunidades con Louvain. Tambi√©n genera una visualizaci√≥n b√°sica.

#### ‚úèÔ∏è Estad√≠sticas Calculadas:
-  **Top 5 usuarios que m√°s mencionan** (grado de salida).
-  **Top 5 usuarios m√°s mencionados** (grado de entrada).
-  **Top 5 en centralidad de intermediaci√≥n** (usuarios puente).
-  **Top 5 en centralidad de cercan√≠a** (usuarios accesibles).
-  **Top 5 en centralidad de autovector** (usuarios con prestigio estructural).
-  **Detecci√≥n de comunidades** con Louvain (requiere red no dirigida).

####  Visualizaci√≥n:
Se genera un grafo con `matplotlib` usando `spring_layout` y se dibujan nodos y aristas con bajo nivel de detalle.

####  Resultados Guardados:
- `self.top_out_degree`
- `self.top_in_degree`
- `self.top_betweenness`
- `self.top_closeness`
- `self.top_eigenvector`

---

## 4. Chat con Modelo de Lenguaje Local

### M√©todo: `chat_local_llm`

```python
def chat_local_llm(self, prompt: str = None)
```

Carga e interact√∫a con un modelo de lenguaje local (`google/gemma-2-2b-it`) en consola.

####  Flujo:
- Carga el modelo y el tokenizador desde HuggingFace.
- Usa GPU si est√° disponible.
- Permite ingresar un `prompt` o interactuar din√°micamente por consola.
- Imprime la respuesta generada por el modelo.

---

## 5. Generaci√≥n de Prompts desde la Red

### M√©todo: `generate_prompt_from_network`

```python
def generate_prompt_from_network(self, G: nx.DiGraph) -> str
```

Genera un prompt en lenguaje natural basado en los an√°lisis de red para alimentar un modelo LLM.

####  Detalles:
- Usa el top 3 de usuarios con mayor grado de salida.
- Usa el hashtag m√°s frecuente extra√≠do del an√°lisis extendido.
- Devuelve un string con una pregunta interpretativa lista para ser usada con un LLM.

####  Ejemplo de Prompt:

```
Se ha analizado una red de interacciones en Twitter basada en menciones...

Los 3 usuarios m√°s activos...
El hashtag m√°s frecuente...
¬øQu√© factores podr√≠an explicar...
```

---

